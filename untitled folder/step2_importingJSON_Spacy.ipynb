{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modern_NLP_in_Python.ipynb       step2_importingJSON_Spacy.ipynb\r\n",
      "step1_munging_and_eda.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_directory = os.path.join('../data', 'mungedASAM.json')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [i for i in range(7843)]\n",
    "events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ids = set(events);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This works but I think might be slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_descriptions=set()\n",
    "data_directory=os.path.join('../data', 'mungedASAM.json')  \n",
    "with codecs.open(data_directory, encoding='utf_8') as f:\n",
    "     for event_json in f:\n",
    "        event = json.loads(event_json)\n",
    "        for item in event:         \n",
    "           event_descriptions.add(item[u'description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('../data')\n",
    "\n",
    "event_txt_filepath = os.path.join(intermediate_directory,\n",
    "                                   'all_descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.29 s, sys: 363 ms, total: 2.65 s\n",
      "Wall time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this took a while. make the if statement True to run; otherwise keep it off.\n",
    "\n",
    "if 1 == 1:\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    with codecs.open(event_txt_filepath, 'w', encoding='utf_8') as event_txt_file:\n",
    "        with codecs.open(data_directory, encoding='utf_8') as event_json_file:\n",
    "            for event_json in event_json_file:                \n",
    "                event = json.loads(event_json)\n",
    "                for item in event:\n",
    "                    event_txt_file.write(item[u'description'].replace('\\n', '\\\\n') + '\\n')\n",
    "                    count += 1\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with codecs.open(event_txt_filepath, encoding='utf_8') as event_txt_file:\n",
    "        for event_count, line in enumerate(event_txt_file):\n",
    "            pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get something going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PANAMA: On 13 July, a monohull with two persons onboard departed Linton Bay, Panama, for the San Blas Islands, Panama. Engine issues caused the yacht to make an unplanned stop in Bahia Nombre de Dios, where no other boats were anchored. The necessary repairs were completed but the hour was late and entering the San Blas in the dark was not an option. The crew planned to lock themselves in securely for the night. They were down below and surprised just after sunset by noise on deck. The captain went up to the cockpit and was ambushed and then restrained by two swimmers who had boarded and concealed themselves behind opposite sides of the spray hood. Immediately a boat with four other men arrived and the captain was held at gunpoint. For 45 minutes the men ransacked and inspected every space in the yacht, stealing cash, credit cards, phones, cameras and video equipment, laptops, a drone, and other items. Some of the men appeared to be high on drugs and were only marginally under the control of the armed leader. Two of the men forced the crew member back into a cabin and indicated they intended to rape her, but the leader prevented this. Once they had collected all things of value, all six men departed in their boat.  (wwwsafetyandsecuritynetorg)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(event_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print (sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.2 ms, sys: 5.37 ms, total: 61.6 ms\n",
      "Wall time: 60.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "PANAMA:\n",
      "\n",
      "Sentence 2:\n",
      "On 13 July, a monohull with two persons onboard departed Linton Bay, Panama, for the San Blas Islands, Panama.\n",
      "\n",
      "Sentence 3:\n",
      "Engine issues caused the yacht to make an unplanned stop in Bahia Nombre de Dios, where no other boats were anchored.\n",
      "\n",
      "Sentence 4:\n",
      "The necessary repairs were completed but the hour was late and entering the San Blas in the dark was not an option.\n",
      "\n",
      "Sentence 5:\n",
      "The crew planned to lock themselves in securely for the night.\n",
      "\n",
      "Sentence 6:\n",
      "They were down below and surprised just after sunset by noise on deck.\n",
      "\n",
      "Sentence 7:\n",
      "The captain went up to the cockpit and was ambushed and then restrained by two swimmers who had boarded and concealed themselves behind opposite sides of the spray hood.\n",
      "\n",
      "Sentence 8:\n",
      "Immediately a boat with four other men arrived and the captain was held at gunpoint.\n",
      "\n",
      "Sentence 9:\n",
      "For 45 minutes the men ransacked and inspected every space in the yacht, stealing cash, credit cards, phones, cameras and video equipment, laptops, a drone, and other items.\n",
      "\n",
      "Sentence 10:\n",
      "Some of the men appeared to be high on drugs and were only marginally under the control of the armed leader.\n",
      "\n",
      "Sentence 11:\n",
      "Two of the men forced the crew member back into a cabin and indicated they intended to rape her, but the leader prevented this.\n",
      "\n",
      "Sentence 12:\n",
      "Once they had collected all things of value, all six men departed in their boat.  \n",
      "\n",
      "Sentence 13:\n",
      "(wwwsafetyandsecuritynetorg)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print ('Sentence {}:'.format(num + 1))\n",
    "    print (sentence)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: 13 July - DATE\n",
      "\n",
      "Entity 2: two - CARDINAL\n",
      "\n",
      "Entity 3: Linton Bay - PERSON\n",
      "\n",
      "Entity 4: Panama - GPE\n",
      "\n",
      "Entity 5: the San Blas Islands - LOC\n",
      "\n",
      "Entity 6: Panama - GPE\n",
      "\n",
      "Entity 7: Bahia Nombre de Dios - ORG\n",
      "\n",
      "Entity 8: the hour - TIME\n",
      "\n",
      "Entity 9: the San Blas - LOC\n",
      "\n",
      "Entity 10: the night - TIME\n",
      "\n",
      "Entity 11: two - CARDINAL\n",
      "\n",
      "Entity 12: four - CARDINAL\n",
      "\n",
      "Entity 13: 45 minutes - TIME\n",
      "\n",
      "Entity 14: Two - CARDINAL\n",
      "\n",
      "Entity 15: six - CARDINAL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print ('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>On</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>July</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>wwwsafetyandsecuritynetorg</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     token_text part_of_speech\n",
       "0                        PANAMA           NOUN\n",
       "1                             :          PUNCT\n",
       "2                            On            ADP\n",
       "3                            13            NUM\n",
       "4                          July          PROPN\n",
       "..                          ...            ...\n",
       "245                                      SPACE\n",
       "246                           (          PUNCT\n",
       "247  wwwsafetyandsecuritynetorg          PROPN\n",
       "248                           )          PUNCT\n",
       "249                          \\n          SPACE\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_pos),\n",
    "             columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>panama</td>\n",
       "      <td>XXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>On</td>\n",
       "      <td>on</td>\n",
       "      <td>Xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>July</td>\n",
       "      <td>July</td>\n",
       "      <td>Xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>wwwsafetyandsecuritynetorg</td>\n",
       "      <td>wwwsafetyandsecuritynetorg</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     token_text                 token_lemma token_shape\n",
       "0                        PANAMA                      panama        XXXX\n",
       "1                             :                           :           :\n",
       "2                            On                          on          Xx\n",
       "3                            13                          13          dd\n",
       "4                          July                        July        Xxxx\n",
       "..                          ...                         ...         ...\n",
       "245                                                                    \n",
       "246                           (                           (           (\n",
       "247  wwwsafetyandsecuritynetorg  wwwsafetyandsecuritynetorg        xxxx\n",
       "248                           )                           )           )\n",
       "249                          \\n                          \\n          \\n\n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_lemma, token_shape),\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>On</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>DATE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>July</td>\n",
       "      <td>DATE</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>(</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>wwwsafetyandsecuritynetorg</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>)</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     token_text entity_type inside_outside_begin\n",
       "0                        PANAMA                                O\n",
       "1                             :                                O\n",
       "2                            On                                O\n",
       "3                            13        DATE                    B\n",
       "4                          July        DATE                    I\n",
       "..                          ...         ...                  ...\n",
       "245                                                            O\n",
       "246                           (                                O\n",
       "247  wwwsafetyandsecuritynetorg                                O\n",
       "248                           )                                O\n",
       "249                          \\n                                O\n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_entity_type, token_entity_iob),\n",
    "             columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>On</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>July</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td></td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>(</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>wwwsafetyandsecuritynetorg</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>)</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>\\n</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           text  log_probability stop? punctuation?  \\\n",
       "0                        PANAMA            -20.0                      \n",
       "1                             :            -20.0                Yes   \n",
       "2                            On            -20.0   Yes                \n",
       "3                            13            -20.0                      \n",
       "4                          July            -20.0                      \n",
       "..                          ...              ...   ...          ...   \n",
       "245                                        -20.0                      \n",
       "246                           (            -20.0                Yes   \n",
       "247  wwwsafetyandsecuritynetorg            -20.0                      \n",
       "248                           )            -20.0                Yes   \n",
       "249                          \\n            -20.0                      \n",
       "\n",
       "    whitespace? number? out of vocab.?  \n",
       "0                                  Yes  \n",
       "1                                  Yes  \n",
       "2                                  Yes  \n",
       "3                   Yes            Yes  \n",
       "4                                  Yes  \n",
       "..          ...     ...            ...  \n",
       "245         Yes                    Yes  \n",
       "246                                Yes  \n",
       "247                                Yes  \n",
       "248                                Yes  \n",
       "249         Yes                    Yes  \n",
       "\n",
       "[250 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4): # I have an old macbook. But I love it.\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.8 s, sys: 14.5 s, total: 1min 12s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(event_txt_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMB\n",
      "\n",
      "peru\n",
      "\n",
      "on 14 April five robber board a bulk carrier near position 12 01s 077 11w Callao Anchorage\n",
      "\n",
      "a duty crewman on routine round be beat\n",
      "\n",
      "alarm be raise and crew muster\n",
      "\n",
      "see the alert crew the robber escape with the duty crewman¿s personal belonging\n",
      "\n",
      "Incident report to local authority who board the vessel for further investigation\n",
      "\n",
      "IMB\n",
      "\n",
      "TRINIDAD and tobago\n",
      "\n",
      "on 14 April a 53-ft u.s.flagged mono hull transiting from Trinidad to Grenada with two person onboard be approach by 8 pirate in an open piroque near position 11 24n 061 36w 16 nm north northeast of the Hibiscus gas platform and 37 mile south of Grenada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print (u' '.join(unigram_sentence))\n",
    "    print (u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 s, sys: 33.7 ms, total: 1.2 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 s, sys: 19.6 ms, total: 2.09 s\n",
      "Wall time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMB\n",
      "\n",
      "peru\n",
      "\n",
      "on 14 April five robber board a bulk_carrier near_position 12_01s 077_11w Callao_Anchorage\n",
      "\n",
      "a duty_crewman on_routine round be beat\n",
      "\n",
      "alarm be raise and crew_muster\n",
      "\n",
      "see the alert_crew the robber escape with the duty crewman¿s personal_belonging\n",
      "\n",
      "Incident_report to local authority who board the vessel for further_investigation\n",
      "\n",
      "IMB\n",
      "\n",
      "TRINIDAD and tobago\n",
      "\n",
      "on 14 April a 53-ft u.s.flagged mono hull transiting from Trinidad to Grenada with two person onboard be approach by 8 pirate in an open piroque near_position 11 24n 061 36w 16 nm_north northeast_of the Hibiscus gas platform and 37 mile_south of Grenada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print (u' '.join(bigram_sentence))\n",
    "    print (u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(intermediate_directory, 'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.13 s, sys: 29.3 ms, total: 1.16 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(intermediate_directory, 'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 21 ms, total: 1.99 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMB\n",
      "\n",
      "peru\n",
      "\n",
      "on 14 April five robber board a bulk_carrier near_position_12_01s 077_11w Callao_Anchorage\n",
      "\n",
      "a duty_crewman_on_routine round be beat\n",
      "\n",
      "alarm be raise and crew_muster\n",
      "\n",
      "see the alert_crew the robber escape_with the duty crewman¿s personal_belonging\n",
      "\n",
      "Incident_report_to local_authority who board the vessel for_further_investigation\n",
      "\n",
      "IMB\n",
      "\n",
      "TRINIDAD and tobago\n",
      "\n",
      "on 14 April a 53-ft u.s.flagged mono hull transiting from Trinidad to Grenada with two person_onboard be approach by 8 pirate in an open piroque near_position 11 24n 061 36w 16 nm_north_northeast_of the Hibiscus gas platform and 37 mile_south_of Grenada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 230, 240):\n",
    "    print (u' '.join(trigram_sentence))\n",
    "    print (u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory, 'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz#egg=en_core_web_sm==2.2.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: spacy>=2.2.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.2.0) (2.2.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.20)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.17.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.9.6)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (41.6.0.post20191030)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/alex/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/alex/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/alex/anaconda3/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.35.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/alex/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alex/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/alex/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/alex/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.25.3)\n",
      "Requirement already satisfied: more-itertools in /Users/alex/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (7.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/alex/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/Users/alex/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1a402ef7f0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -m spacy download en\n",
    "spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.7/runpy.py:193: DeprecationWarning: [W016] The keyword argument `n_threads` on the is now deprecated, as the v2.x models cannot release the global interpreter lock. Future versions may introduce a `n_process` argument for parallel inference via multiprocessing.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy' has no attribute 'en'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'spacy' has no attribute 'en'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(event_txt_filepath),\n",
    "                                      batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                              if term not in spacy.en.STOPWORDS]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This wasn't all that helpful. Bigrams were ok. Might work with some more training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = os.path.join(intermediate_directory, 'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 929 µs, sys: 1.32 ms, total: 2.25 ms\n",
      "Wall time: 1.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to learn the dictionary yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=2, no_above=0.01)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join(intermediate_directory, 'trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 ms, sys: 2.5 ms, total: 3.85 ms\n",
      "Wall time: 2.21 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to build the bag-of-words corpus yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                       trigram_bow_generator(trigram_reviews_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/lda_model_all'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')\n",
    "lda_model_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot compute LDA over an empty collection (no terms)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute LDA over an empty collection (no terms)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute LDA over an empty collection (no terms)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=50,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=3) # set to your number of physical cores minus one\n",
    "    \n",
    "    lda.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
